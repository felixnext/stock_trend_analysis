{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Value Prediction\n",
    "\n",
    "In this Notebook, we will create the actual prediction system, by testing various approaches and accuracy against multiple time-horizons (target_days variable).\n",
    "\n",
    "First we will load all libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "from datetime import datetime\n",
    "sys.path.insert(1, '..')\n",
    "import recommender as rcmd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# classification approaches\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# regression approaches\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# data handling and scoring\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the input data pipelines for stock and statement data. Therefore we will have to split data into training and test sets. There are two options for doing that:\n",
    "\n",
    "* Splitting the list of symbols\n",
    "* Splitting the results list of training stock datapoints\n",
    "\n",
    "We will use the first option in order ensure a clear split (since the generate data has overlapping time frames, the second options would generate data that might have been seen by the system beforehand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5  -0.25  0.    0.25  0.5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\recommender\\learning\\preprocess.py:252: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  df['expenses_research_netcash'] = np.divide(df['expenses_research'], df['cash_net'])\n",
      "..\\recommender\\learning\\preprocess.py:252: RuntimeWarning: invalid value encountered in true_divide\n",
      "  df['expenses_research_netcash'] = np.divide(df['expenses_research'], df['cash_net'])\n",
      "..\\recommender\\learning\\preprocess.py:311: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  df['pe_ratio'] = np.divide(df[col_price], df['eps_diluted'])\n",
      "..\\recommender\\learning\\preprocess.py:312: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  df['cash_share'] = np.divide(df['cash_net'], np.divide(df['shareholder_equity'], df[col_price]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5  -0.25  0.    0.25  0.5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\recommender\\learning\\preprocess.py:252: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  df['expenses_research_netcash'] = np.divide(df['expenses_research'], df['cash_net'])\n",
      "..\\recommender\\learning\\preprocess.py:311: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  df['pe_ratio'] = np.divide(df[col_price], df['eps_diluted'])\n",
      "..\\recommender\\learning\\preprocess.py:312: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  df['cash_share'] = np.divide(df['cash_net'], np.divide(df['shareholder_equity'], df[col_price]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42542, 35)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_1</th>\n",
       "      <th>day_2</th>\n",
       "      <th>day_3</th>\n",
       "      <th>day_4</th>\n",
       "      <th>day_5</th>\n",
       "      <th>day_6</th>\n",
       "      <th>day_7</th>\n",
       "      <th>day_8</th>\n",
       "      <th>day_9</th>\n",
       "      <th>day_10</th>\n",
       "      <th>...</th>\n",
       "      <th>dividend_share_growth_10y</th>\n",
       "      <th>revenue_share_growth_3y</th>\n",
       "      <th>revenue_share_growth_5y</th>\n",
       "      <th>revenue_share_growth_10y</th>\n",
       "      <th>pe_ratio</th>\n",
       "      <th>cash_share</th>\n",
       "      <th>norm_price</th>\n",
       "      <th>target</th>\n",
       "      <th>target_cat</th>\n",
       "      <th>symbol</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-05-29</th>\n",
       "      <td>-0.074448</td>\n",
       "      <td>-0.032145</td>\n",
       "      <td>-0.034584</td>\n",
       "      <td>-0.060832</td>\n",
       "      <td>-0.066147</td>\n",
       "      <td>-0.055590</td>\n",
       "      <td>-0.081219</td>\n",
       "      <td>-0.063635</td>\n",
       "      <td>-0.052241</td>\n",
       "      <td>-0.030580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0632</td>\n",
       "      <td>0.0637</td>\n",
       "      <td>0.1173</td>\n",
       "      <td>40.395588</td>\n",
       "      <td>37.358140</td>\n",
       "      <td>27.4690</td>\n",
       "      <td>0.123134</td>\n",
       "      <td>3</td>\n",
       "      <td>ACN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-29</th>\n",
       "      <td>-0.074182</td>\n",
       "      <td>0.005387</td>\n",
       "      <td>0.051057</td>\n",
       "      <td>0.042934</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.062329</td>\n",
       "      <td>0.043763</td>\n",
       "      <td>0.046747</td>\n",
       "      <td>0.077414</td>\n",
       "      <td>0.076668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>-0.0329</td>\n",
       "      <td>0.0296</td>\n",
       "      <td>-0.0178</td>\n",
       "      <td>44.685183</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>12.0650</td>\n",
       "      <td>0.144475</td>\n",
       "      <td>3</td>\n",
       "      <td>APOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-29</th>\n",
       "      <td>-0.148598</td>\n",
       "      <td>-0.148598</td>\n",
       "      <td>-0.056340</td>\n",
       "      <td>-0.056340</td>\n",
       "      <td>-0.104358</td>\n",
       "      <td>-0.104358</td>\n",
       "      <td>-0.096529</td>\n",
       "      <td>-0.096529</td>\n",
       "      <td>-0.048127</td>\n",
       "      <td>-0.048127</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>-0.0995</td>\n",
       "      <td>-0.0553</td>\n",
       "      <td>-0.0162</td>\n",
       "      <td>-2.099310</td>\n",
       "      <td>-0.256086</td>\n",
       "      <td>1.8264</td>\n",
       "      <td>-0.012180</td>\n",
       "      <td>2</td>\n",
       "      <td>BSET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-29</th>\n",
       "      <td>-0.002762</td>\n",
       "      <td>0.058128</td>\n",
       "      <td>0.093246</td>\n",
       "      <td>0.084071</td>\n",
       "      <td>0.105302</td>\n",
       "      <td>0.038836</td>\n",
       "      <td>0.019425</td>\n",
       "      <td>0.011060</td>\n",
       "      <td>0.023155</td>\n",
       "      <td>0.044372</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0024</td>\n",
       "      <td>-0.0427</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0634</td>\n",
       "      <td>28.968078</td>\n",
       "      <td>0.759693</td>\n",
       "      <td>7.5317</td>\n",
       "      <td>0.423535</td>\n",
       "      <td>4</td>\n",
       "      <td>EBF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-29</th>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>-0.057143</td>\n",
       "      <td>-0.057143</td>\n",
       "      <td>-0.114286</td>\n",
       "      <td>-0.114286</td>\n",
       "      <td>-0.028571</td>\n",
       "      <td>-0.028571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.1190</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>-0.0085</td>\n",
       "      <td>1.093750</td>\n",
       "      <td>9.149215</td>\n",
       "      <td>1.4000</td>\n",
       "      <td>-0.202597</td>\n",
       "      <td>2</td>\n",
       "      <td>EMMS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               day_1     day_2     day_3     day_4     day_5     day_6  \\\n",
       "date                                                                     \n",
       "2009-05-29 -0.074448 -0.032145 -0.034584 -0.060832 -0.066147 -0.055590   \n",
       "2009-05-29 -0.074182  0.005387  0.051057  0.042934  0.070700  0.062329   \n",
       "2009-05-29 -0.148598 -0.148598 -0.056340 -0.056340 -0.104358 -0.104358   \n",
       "2009-05-29 -0.002762  0.058128  0.093246  0.084071  0.105302  0.038836   \n",
       "2009-05-29  0.028571  0.028571 -0.057143 -0.057143 -0.114286 -0.114286   \n",
       "\n",
       "               day_7     day_8     day_9    day_10  ...  \\\n",
       "date                                                ...   \n",
       "2009-05-29 -0.081219 -0.063635 -0.052241 -0.030580  ...   \n",
       "2009-05-29  0.043763  0.046747  0.077414  0.076668  ...   \n",
       "2009-05-29 -0.096529 -0.096529 -0.048127 -0.048127  ...   \n",
       "2009-05-29  0.019425  0.011060  0.023155  0.044372  ...   \n",
       "2009-05-29 -0.028571 -0.028571  0.000000  0.000000  ...   \n",
       "\n",
       "            dividend_share_growth_10y  revenue_share_growth_3y  \\\n",
       "date                                                             \n",
       "2009-05-29                     0.0000                   0.0632   \n",
       "2009-05-29                     0.0450                  -0.0329   \n",
       "2009-05-29                    -1.0000                  -0.0995   \n",
       "2009-05-29                    -0.0024                  -0.0427   \n",
       "2009-05-29                     0.0000                  -0.1190   \n",
       "\n",
       "            revenue_share_growth_5y  revenue_share_growth_10y   pe_ratio  \\\n",
       "date                                                                       \n",
       "2009-05-29                   0.0637                    0.1173  40.395588   \n",
       "2009-05-29                   0.0296                   -0.0178  44.685183   \n",
       "2009-05-29                  -0.0553                   -0.0162  -2.099310   \n",
       "2009-05-29                   0.0081                    0.0634  28.968078   \n",
       "2009-05-29                   0.0137                   -0.0085   1.093750   \n",
       "\n",
       "            cash_share  norm_price    target  target_cat  symbol  \n",
       "date                                                              \n",
       "2009-05-29   37.358140     27.4690  0.123134           3     ACN  \n",
       "2009-05-29    0.843000     12.0650  0.144475           3    APOG  \n",
       "2009-05-29   -0.256086      1.8264 -0.012180           2    BSET  \n",
       "2009-05-29    0.759693      7.5317  0.423535           4     EBF  \n",
       "2009-05-29    9.149215      1.4000 -0.202597           2    EMMS  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create cache object\n",
    "cache = rcmd.stocks.Cache()\n",
    "\n",
    "# load list of all available stocks and sample sub-list\n",
    "stocks = cache.list_data('stock')\n",
    "\n",
    "def train_test_data(back, ahead, xlim, split=0.3, count=2000, stocks=stocks, cache=cache):\n",
    "    '''Generetes a train test split'''\n",
    "    sample = np.random.choice(list(stocks.keys()), 2000)\n",
    "    # split the stock data\n",
    "    count_train = int((1-split) * count)\n",
    "    sample_train = sample[:count_train]\n",
    "    sample_test = sample[count_train:]\n",
    "\n",
    "    # generate sample data\n",
    "    df_train = rcmd.learning.preprocess.create_dataset(sample_train, stocks, cache, back, ahead, xlim)\n",
    "    df_test = rcmd.learning.preprocess.create_dataset(sample_test, stocks, cache, back, ahead, xlim)\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "df_train, df_test = train_test_data(14, 66, (-.5, .5))\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded and split the data, we have to divide it into input and output data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2009-05-29    1\n",
       "2009-05-29    1\n",
       "2009-05-29    1\n",
       "2009-05-29    1\n",
       "2009-05-29    1\n",
       "2009-05-29    1\n",
       "2009-05-29    1\n",
       "2009-05-29    1\n",
       "2009-05-29    1\n",
       "2009-05-29    1\n",
       "2009-05-29    1\n",
       "2009-06-12    1\n",
       "2009-06-12    1\n",
       "2009-06-12    1\n",
       "2009-06-15    1\n",
       "2009-06-15    1\n",
       "2009-06-16    1\n",
       "2009-06-16    1\n",
       "2009-06-16    1\n",
       "2009-06-16    1\n",
       "2009-06-17    1\n",
       "2009-06-17    1\n",
       "2009-06-17    1\n",
       "2009-06-17    1\n",
       "2009-06-17    1\n",
       "2009-06-17    1\n",
       "2009-06-18    1\n",
       "2009-06-18    1\n",
       "2009-06-18    1\n",
       "2009-06-19    1\n",
       "             ..\n",
       "2017-08-31    1\n",
       "2017-08-31    1\n",
       "2017-08-31    1\n",
       "2017-08-31    1\n",
       "2017-08-31    1\n",
       "2017-08-31    1\n",
       "2017-08-31    1\n",
       "2017-08-31    1\n",
       "2017-08-31    1\n",
       "2017-08-31    1\n",
       "2017-08-31    1\n",
       "2017-09-01    1\n",
       "2017-09-05    1\n",
       "2017-09-05    1\n",
       "2017-09-06    1\n",
       "2017-09-06    1\n",
       "2017-09-06    1\n",
       "2017-09-06    1\n",
       "2017-09-07    1\n",
       "2017-09-07    1\n",
       "2017-09-08    1\n",
       "2017-09-11    1\n",
       "2017-09-11    1\n",
       "2017-09-11    1\n",
       "2017-09-11    1\n",
       "2017-09-12    1\n",
       "2017-09-12    1\n",
       "2017-09-12    1\n",
       "2017-09-12    1\n",
       "2017-09-13    1\n",
       "Length: 42542, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def divide_data(df, xlim):\n",
    "    '''Splits the data into 3 sets: input, ouput_classify, output_regression.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DF to contain all relevant data\n",
    "        xlim (tuple): tuple of integers used to clip and scale regression values to a range of 0 to 1\n",
    "        \n",
    "    Returns:\n",
    "        df_X: DataFrame with input values\n",
    "        df_y_cls: DataFrame with classification labels\n",
    "        df_y_reg: DataFrame with regression values\n",
    "    '''\n",
    "    # remove all target cols\n",
    "    df_X = df.drop(['target', 'target_cat', 'norm_price', 'symbol'], axis=1)\n",
    "    # convert to dummy classes\n",
    "    df_y_cls = pd.get_dummies(df['target_cat'], prefix='cat', dummy_na=False)\n",
    "    # clip values and scale to vals\n",
    "    df_y_reg = np.divide( np.subtract( df['target'].clip(xlim[0], xlim[1]), xlim[0] ), (xlim[1] - xlim[0]) )\n",
    "    \n",
    "    return df_X, df_y_cls, df_y_reg\n",
    "\n",
    "X_train, y_ctrain, y_rtrain = divide_data(df_train, (-.5, .5))\n",
    "X_test, y_ctest, y_rtest = divide_data(df_test, (-.5, .5))\n",
    "y_ctrain.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we create the actual prediction systems, we will have to define metrics, how we want to measure the success of the systems.\n",
    "As we have two approaches (classification and regression) we will use two types metrics:\n",
    "\n",
    "* Precision, Recall & Accuracy\n",
    "* RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _metric_classifier(y_true, y_pred, avg=None):\n",
    "    p = precision_score(y_true, y_pred, average=avg)\n",
    "    r = recall_score(y_true, y_pred, average=avg)\n",
    "    f1 = f1_score(y_true, y_pred, average=avg)\n",
    "    return f1, p, r\n",
    "\n",
    "def score_classifier(y_true, y_pred):\n",
    "    '''Calculates the relevant scores for a classifer and outputs. This should show predicitions per class.'''\n",
    "    f1, p, r = _metric_classifier(y_true, y_pred, avg='micro')\n",
    "    \n",
    "    print(\"Model Performance: F1={:.4f} (P={:.4f} / R={:.4f})\".format(f1, p, r))\n",
    "    # list scores of single classes\n",
    "    for i, c in enumerate(y_true.columns):\n",
    "        sf1, sp, sr = _metric_classifier(y_true.iloc[:, i], y_pred[:, i], avg='binary')\n",
    "        print(\"  {:10} F1={:.4f} (P={:.4f} / R={:.4f})\".format(c + \":\", sf1, sp, sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "The first step is to create a baseline for both approaches (classification and regression). In case of regression our target value will be `target` and for classification it will be `target_cat` (which we might convert into a one-hot vector along the way).\n",
    "\n",
    "Lets start with the simpler form of classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat_0      243\n",
       "cat_1     2047\n",
       "cat_2    15655\n",
       "cat_3    20405\n",
       "cat_4     3126\n",
       "cat_5     1066\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ctrain.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance: F1=0.2655 (P=0.4987 / R=0.1809)\n",
      "  cat_0:     F1=0.0099 (P=0.0122 / R=0.0083)\n",
      "  cat_1:     F1=0.0138 (P=0.0654 / R=0.0077)\n",
      "  cat_2:     F1=0.0116 (P=0.5735 / R=0.0059)\n",
      "  cat_3:     F1=0.4302 (P=0.5175 / R=0.3682)\n",
      "  cat_4:     F1=0.0026 (P=0.0455 / R=0.0014)\n",
      "  cat_5:     F1=0.0000 (P=0.0000 / R=0.0000)\n"
     ]
    }
   ],
   "source": [
    "# scale input data to improve convergance (Note: scaler has to be used for other input data as well)\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# train element\n",
    "classifier = MultiOutputClassifier(LogisticRegression(max_iter=500, solver='lbfgs'))\n",
    "classifier.fit(X_train_std, y_ctrain)\n",
    "\n",
    "# predict data\n",
    "y_pred = classifier.predict(X_test_std)\n",
    "\n",
    "score_classifier(y_ctest, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a strong bias in the system for `cat_3`, which also has the highest number of training samples. Future work might include oversampling or more target datapoint selection to reduce these biases. In order to capture more information from the data point (which tend to have a gaussian distribution as we have seen), we will try gaussian mixture models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_gmm = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, support vector machines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance: F1=0.4157 (P=0.5416 / R=0.3372)\n",
      "  cat_0:     F1=0.0385 (P=1.0000 / R=0.0196)\n",
      "  cat_1:     F1=0.0154 (P=1.0000 / R=0.0078)\n",
      "  cat_2:     F1=0.0107 (P=0.4932 / R=0.0054)\n",
      "  cat_3:     F1=0.6126 (P=0.5414 / R=0.7053)\n",
      "  cat_4:     F1=0.0027 (P=1.0000 / R=0.0014)\n",
      "  cat_5:     F1=0.0000 (P=0.0000 / R=0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\envs\\ds-stocks\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\programming\\envs\\ds-stocks\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "classifier_svm = MultiOutputClassifier(SVC())\n",
    "classifier_svm.fit(X_train_std, y_ctrain)\n",
    "y_pred_svm = classifier_svm.predict(X_test_std)\n",
    "score_classifier(y_ctest, y_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the results improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42542, 31)\n",
      "Train on 42542 samples\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001C640DCE488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001C640DCE488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "42542/42542 [==============================] - 4s 84us/sample - loss: 1.1531 - accuracy: 0.4854\n",
      "Epoch 2/100\n",
      "42542/42542 [==============================] - 3s 70us/sample - loss: 1.1172 - accuracy: 0.4941\n",
      "Epoch 3/100\n",
      "42542/42542 [==============================] - 3s 81us/sample - loss: 1.1088 - accuracy: 0.4949\n",
      "Epoch 4/100\n",
      "42542/42542 [==============================] - 4s 90us/sample - loss: 1.1019 - accuracy: 0.4997\n",
      "Epoch 5/100\n",
      "42542/42542 [==============================] - 5s 111us/sample - loss: 1.0935 - accuracy: 0.5012\n",
      "Epoch 6/100\n",
      "42542/42542 [==============================] - 5s 114us/sample - loss: 1.0868 - accuracy: 0.5025\n",
      "Epoch 7/100\n",
      "42542/42542 [==============================] - 4s 84us/sample - loss: 1.0799 - accuracy: 0.5063\n",
      "Epoch 8/100\n",
      "42542/42542 [==============================] - 4s 90us/sample - loss: 1.0711 - accuracy: 0.5069\n",
      "Epoch 9/100\n",
      "42542/42542 [==============================] - 4s 84us/sample - loss: 1.0607 - accuracy: 0.5147\n",
      "Epoch 10/100\n",
      "42542/42542 [==============================] - 4s 96us/sample - loss: 1.0489 - accuracy: 0.5195\n",
      "Epoch 11/100\n",
      "42542/42542 [==============================] - 4s 104us/sample - loss: 1.0353 - accuracy: 0.5243\n",
      "Epoch 12/100\n",
      "42542/42542 [==============================] - 4s 106us/sample - loss: 1.0225 - accuracy: 0.5308\n",
      "Epoch 13/100\n",
      "42542/42542 [==============================] - 4s 96us/sample - loss: 1.0060 - accuracy: 0.5384\n",
      "Epoch 14/100\n",
      "42542/42542 [==============================] - 4s 88us/sample - loss: 0.9885 - accuracy: 0.5468\n",
      "Epoch 15/100\n",
      "42542/42542 [==============================] - 4s 88us/sample - loss: 0.9722 - accuracy: 0.5536\n",
      "Epoch 16/100\n",
      "42542/42542 [==============================] - 4s 90us/sample - loss: 0.9523 - accuracy: 0.5638\n",
      "Epoch 17/100\n",
      "42542/42542 [==============================] - 4s 92us/sample - loss: 0.9365 - accuracy: 0.5718\n",
      "Epoch 18/100\n",
      "42542/42542 [==============================] - 4s 85us/sample - loss: 0.9144 - accuracy: 0.5842\n",
      "Epoch 19/100\n",
      "42542/42542 [==============================] - 4s 93us/sample - loss: 0.8935 - accuracy: 0.5948\n",
      "Epoch 20/100\n",
      "42542/42542 [==============================] - 4s 87us/sample - loss: 0.8759 - accuracy: 0.6054\n",
      "Epoch 21/100\n",
      "42542/42542 [==============================] - 4s 87us/sample - loss: 0.8558 - accuracy: 0.6148\n",
      "Epoch 22/100\n",
      "42542/42542 [==============================] - 4s 89us/sample - loss: 0.8323 - accuracy: 0.6250\n",
      "Epoch 23/100\n",
      "42542/42542 [==============================] - 4s 90us/sample - loss: 0.8129 - accuracy: 0.6353\n",
      "Epoch 24/100\n",
      "42542/42542 [==============================] - 4s 87us/sample - loss: 0.7935 - accuracy: 0.6458\n",
      "Epoch 25/100\n",
      "42542/42542 [==============================] - 4s 98us/sample - loss: 0.7729 - accuracy: 0.6553\n",
      "Epoch 26/100\n",
      "42542/42542 [==============================] - 4s 87us/sample - loss: 0.7562 - accuracy: 0.6633\n",
      "Epoch 27/100\n",
      "42542/42542 [==============================] - 4s 87us/sample - loss: 0.7343 - accuracy: 0.6723\n",
      "Epoch 28/100\n",
      "42542/42542 [==============================] - 4s 88us/sample - loss: 0.7226 - accuracy: 0.6781\n",
      "Epoch 29/100\n",
      "42542/42542 [==============================] - 4s 87us/sample - loss: 0.6981 - accuracy: 0.6896\n",
      "Epoch 30/100\n",
      "42542/42542 [==============================] - 4s 87us/sample - loss: 0.6826 - accuracy: 0.6996\n",
      "Epoch 31/100\n",
      "42542/42542 [==============================] - 4s 88us/sample - loss: 0.6660 - accuracy: 0.7085\n",
      "Epoch 32/100\n",
      "42542/42542 [==============================] - 4s 87us/sample - loss: 0.6481 - accuracy: 0.7139\n",
      "Epoch 33/100\n",
      "42542/42542 [==============================] - 4s 88us/sample - loss: 0.6341 - accuracy: 0.7240\n",
      "Epoch 34/100\n",
      "42542/42542 [==============================] - 4s 88us/sample - loss: 0.6188 - accuracy: 0.7278\n",
      "Epoch 35/100\n",
      "42542/42542 [==============================] - 4s 88us/sample - loss: 0.6099 - accuracy: 0.7349\n",
      "Epoch 36/100\n",
      "42542/42542 [==============================] - 4s 87us/sample - loss: 0.5840 - accuracy: 0.7457\n",
      "Epoch 37/100\n",
      "42542/42542 [==============================] - 6s 143us/sample - loss: 0.5736 - accuracy: 0.7502\n",
      "Epoch 38/100\n",
      "42542/42542 [==============================] - 6s 141us/sample - loss: 0.5612 - accuracy: 0.7564\n",
      "Epoch 39/100\n",
      "42542/42542 [==============================] - 5s 127us/sample - loss: 0.5490 - accuracy: 0.7634\n",
      "Epoch 40/100\n",
      "42542/42542 [==============================] - 5s 119us/sample - loss: 0.5330 - accuracy: 0.7714\n",
      "Epoch 41/100\n",
      "42542/42542 [==============================] - 5s 127us/sample - loss: 0.5225 - accuracy: 0.7764\n",
      "Epoch 42/100\n",
      "42542/42542 [==============================] - 5s 113us/sample - loss: 0.5121 - accuracy: 0.7831\n",
      "Epoch 43/100\n",
      "42542/42542 [==============================] - 4s 93us/sample - loss: 0.4988 - accuracy: 0.7867\n",
      "Epoch 44/100\n",
      "42542/42542 [==============================] - 4s 96us/sample - loss: 0.4900 - accuracy: 0.7931\n",
      "Epoch 45/100\n",
      "42542/42542 [==============================] - 4s 98us/sample - loss: 0.4762 - accuracy: 0.7978\n",
      "Epoch 46/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.4740 - accuracy: 0.7989\n",
      "Epoch 47/100\n",
      "42542/42542 [==============================] - 4s 91us/sample - loss: 0.4571 - accuracy: 0.8063\n",
      "Epoch 48/100\n",
      "42542/42542 [==============================] - 4s 92us/sample - loss: 0.4534 - accuracy: 0.8068\n",
      "Epoch 49/100\n",
      "42542/42542 [==============================] - 4s 90us/sample - loss: 0.4449 - accuracy: 0.8121\n",
      "Epoch 50/100\n",
      "42542/42542 [==============================] - 4s 99us/sample - loss: 0.4258 - accuracy: 0.8225\n",
      "Epoch 51/100\n",
      "42542/42542 [==============================] - 4s 92us/sample - loss: 0.4384 - accuracy: 0.8166\n",
      "Epoch 52/100\n",
      "42542/42542 [==============================] - 4s 91us/sample - loss: 0.4131 - accuracy: 0.8269\n",
      "Epoch 53/100\n",
      "42542/42542 [==============================] - 4s 100us/sample - loss: 0.4055 - accuracy: 0.8301\n",
      "Epoch 54/100\n",
      "42542/42542 [==============================] - 4s 97us/sample - loss: 0.4062 - accuracy: 0.8316\n",
      "Epoch 55/100\n",
      "42542/42542 [==============================] - 4s 102us/sample - loss: 0.3876 - accuracy: 0.8373\n",
      "Epoch 56/100\n",
      "42542/42542 [==============================] - 4s 89us/sample - loss: 0.3972 - accuracy: 0.8367\n",
      "Epoch 57/100\n",
      "42542/42542 [==============================] - 4s 93us/sample - loss: 0.3819 - accuracy: 0.8411\n",
      "Epoch 58/100\n",
      "42542/42542 [==============================] - 4s 92us/sample - loss: 0.3795 - accuracy: 0.8420\n",
      "Epoch 59/100\n",
      "42542/42542 [==============================] - 4s 93us/sample - loss: 0.3719 - accuracy: 0.8467\n",
      "Epoch 60/100\n",
      "42542/42542 [==============================] - 4s 96us/sample - loss: 0.3646 - accuracy: 0.8498\n",
      "Epoch 61/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.3613 - accuracy: 0.8508\n",
      "Epoch 62/100\n",
      "42542/42542 [==============================] - 4s 93us/sample - loss: 0.3662 - accuracy: 0.8507\n",
      "Epoch 63/100\n",
      "42542/42542 [==============================] - 4s 95us/sample - loss: 0.3438 - accuracy: 0.8568\n",
      "Epoch 64/100\n",
      "42542/42542 [==============================] - 4s 96us/sample - loss: 0.3375 - accuracy: 0.8624\n",
      "Epoch 65/100\n",
      "42542/42542 [==============================] - 4s 100us/sample - loss: 0.3443 - accuracy: 0.8601\n",
      "Epoch 66/100\n",
      "42542/42542 [==============================] - 4s 100us/sample - loss: 0.3440 - accuracy: 0.8614\n",
      "Epoch 67/100\n",
      "42542/42542 [==============================] - 4s 97us/sample - loss: 0.3274 - accuracy: 0.8674\n",
      "Epoch 68/100\n",
      "42542/42542 [==============================] - 4s 95us/sample - loss: 0.3149 - accuracy: 0.8709\n",
      "Epoch 69/100\n",
      "42542/42542 [==============================] - 4s 93us/sample - loss: 0.3205 - accuracy: 0.8671\n",
      "Epoch 70/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.3212 - accuracy: 0.8711\n",
      "Epoch 71/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.3238 - accuracy: 0.8700\n",
      "Epoch 72/100\n",
      "42542/42542 [==============================] - 4s 95us/sample - loss: 0.2898 - accuracy: 0.8831\n",
      "Epoch 73/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.3070 - accuracy: 0.8757\n",
      "Epoch 74/100\n",
      "42542/42542 [==============================] - 4s 97us/sample - loss: 0.3022 - accuracy: 0.8788\n",
      "Epoch 75/100\n",
      "42542/42542 [==============================] - 4s 97us/sample - loss: 0.2943 - accuracy: 0.8834\n",
      "Epoch 76/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.2929 - accuracy: 0.8831\n",
      "Epoch 77/100\n",
      "42542/42542 [==============================] - 4s 95us/sample - loss: 0.2907 - accuracy: 0.8831\n",
      "Epoch 78/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.2920 - accuracy: 0.8823\n",
      "Epoch 79/100\n",
      "42542/42542 [==============================] - 4s 95us/sample - loss: 0.2871 - accuracy: 0.8837\n",
      "Epoch 80/100\n",
      "42542/42542 [==============================] - 5s 127us/sample - loss: 0.2744 - accuracy: 0.8897\n",
      "Epoch 81/100\n",
      "42542/42542 [==============================] - 6s 134us/sample - loss: 0.2799 - accuracy: 0.8883\n",
      "Epoch 82/100\n",
      "42542/42542 [==============================] - 7s 154us/sample - loss: 0.2748 - accuracy: 0.8916\n",
      "Epoch 83/100\n",
      "42542/42542 [==============================] - 4s 100us/sample - loss: 0.2737 - accuracy: 0.8919\n",
      "Epoch 84/100\n",
      "42542/42542 [==============================] - 4s 96us/sample - loss: 0.2634 - accuracy: 0.8975\n",
      "Epoch 85/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.2730 - accuracy: 0.8910\n",
      "Epoch 86/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.2512 - accuracy: 0.9009\n",
      "Epoch 87/100\n",
      "42542/42542 [==============================] - 5s 109us/sample - loss: 0.2773 - accuracy: 0.8934\n",
      "Epoch 88/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.2676 - accuracy: 0.8977\n",
      "Epoch 89/100\n",
      "42542/42542 [==============================] - 5s 116us/sample - loss: 0.2532 - accuracy: 0.9015\n",
      "Epoch 90/100\n",
      "42542/42542 [==============================] - 4s 99us/sample - loss: 0.2509 - accuracy: 0.9010\n",
      "Epoch 91/100\n",
      "42542/42542 [==============================] - 4s 102us/sample - loss: 0.2430 - accuracy: 0.9045\n",
      "Epoch 92/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.2454 - accuracy: 0.9057\n",
      "Epoch 93/100\n",
      "42542/42542 [==============================] - 4s 98us/sample - loss: 0.2619 - accuracy: 0.9009\n",
      "Epoch 94/100\n",
      "42542/42542 [==============================] - 4s 93us/sample - loss: 0.2472 - accuracy: 0.9070\n",
      "Epoch 95/100\n",
      "42542/42542 [==============================] - 4s 100us/sample - loss: 0.2335 - accuracy: 0.9083\n",
      "Epoch 96/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.2351 - accuracy: 0.9111\n",
      "Epoch 97/100\n",
      "42542/42542 [==============================] - 4s 99us/sample - loss: 0.2414 - accuracy: 0.9064\n",
      "Epoch 98/100\n",
      "42542/42542 [==============================] - 4s 97us/sample - loss: 0.2448 - accuracy: 0.9054\n",
      "Epoch 99/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.2324 - accuracy: 0.9098\n",
      "Epoch 100/100\n",
      "42542/42542 [==============================] - 4s 100us/sample - loss: 0.2190 - accuracy: 0.9149\n",
      "Model Performance: F1=0.4841 (P=0.4841 / R=0.4841)\n",
      "  cat_0:     F1=0.1386 (P=0.1707 / R=0.1167)\n",
      "  cat_1:     F1=0.2188 (P=0.2192 / R=0.2184)\n",
      "  cat_2:     F1=0.4441 (P=0.4597 / R=0.4296)\n",
      "  cat_3:     F1=0.5946 (P=0.5822 / R=0.6076)\n",
      "  cat_4:     F1=0.2565 (P=0.2559 / R=0.2571)\n",
      "  cat_5:     F1=0.1798 (P=0.1670 / R=0.1947)\n"
     ]
    }
   ],
   "source": [
    "# simple feed forward network\n",
    "print(X_train.shape)\n",
    "classifier_ffn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_std.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(y_ctrain.shape[1], activation=tf.nn.softmax)\n",
    "])\n",
    "classifier_ffn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier_ffn.fit(X_train_std, df_train['target_cat'].to_numpy(), epochs=100)\n",
    "\n",
    "y_pred_ffn = classifier_ffn.predict(X_test_std)\n",
    "y_pred_ffn = pd.get_dummies(y_pred_ffn.argmax(axis=1))\n",
    "print(y_pred_ffn.sum(axis=0))\n",
    "score_classifier(y_ctest, y_pred_ffn.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is noteworthy that the output of the model in the test data resembles the input distribution. Lets try to improve generalization with a more complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42542 samples\n",
      "Epoch 1/400\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001C6349F08C8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001C6349F08C8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "42542/42542 [==============================] - 8s 183us/sample - loss: 1.2917 - accuracy: 0.4405\n",
      "Epoch 2/400\n",
      "42542/42542 [==============================] - 6s 137us/sample - loss: 1.1521 - accuracy: 0.4808\n",
      "Epoch 3/400\n",
      "42542/42542 [==============================] - 6s 137us/sample - loss: 1.1403 - accuracy: 0.4844\n",
      "Epoch 4/400\n",
      "42542/42542 [==============================] - ETA: 0s - loss: 1.1342 - accuracy: 0.48 - 7s 161us/sample - loss: 1.1341 - accuracy: 0.4827\n",
      "Epoch 5/400\n",
      "42542/42542 [==============================] - 7s 172us/sample - loss: 1.1299 - accuracy: 0.4868\n",
      "Epoch 6/400\n",
      "42542/42542 [==============================] - 7s 160us/sample - loss: 1.1275 - accuracy: 0.4853\n",
      "Epoch 7/400\n",
      "42542/42542 [==============================] - 7s 170us/sample - loss: 1.1245 - accuracy: 0.4902\n",
      "Epoch 8/400\n",
      "42542/42542 [==============================] - 7s 173us/sample - loss: 1.1220 - accuracy: 0.4898\n",
      "Epoch 9/400\n",
      "42542/42542 [==============================] - 9s 213us/sample - loss: 1.1207 - accuracy: 0.4892\n",
      "Epoch 10/400\n",
      "42542/42542 [==============================] - 8s 178us/sample - loss: 1.1172 - accuracy: 0.4929\n",
      "Epoch 11/400\n",
      "42542/42542 [==============================] - 7s 161us/sample - loss: 1.1159 - accuracy: 0.4899\n",
      "Epoch 12/400\n",
      "42542/42542 [==============================] - 7s 167us/sample - loss: 1.1142 - accuracy: 0.4902\n",
      "Epoch 13/400\n",
      "42542/42542 [==============================] - 7s 163us/sample - loss: 1.1112 - accuracy: 0.4915\n",
      "Epoch 14/400\n",
      "42542/42542 [==============================] - 7s 166us/sample - loss: 1.1093 - accuracy: 0.4909\n",
      "Epoch 15/400\n",
      "42542/42542 [==============================] - 7s 161us/sample - loss: 1.1086 - accuracy: 0.4956\n",
      "Epoch 16/400\n",
      "42542/42542 [==============================] - 7s 161us/sample - loss: 1.1057 - accuracy: 0.4948\n",
      "Epoch 17/400\n",
      "42542/42542 [==============================] - 7s 163us/sample - loss: 1.1041 - accuracy: 0.4969\n",
      "Epoch 18/400\n",
      "42542/42542 [==============================] - ETA: 0s - loss: 1.1032 - accuracy: 0.49 - 7s 169us/sample - loss: 1.1031 - accuracy: 0.4963\n",
      "Epoch 19/400\n",
      "42542/42542 [==============================] - 7s 164us/sample - loss: 1.1011 - accuracy: 0.4975\n",
      "Epoch 20/400\n",
      "42542/42542 [==============================] - 7s 163us/sample - loss: 1.1002 - accuracy: 0.4964\n",
      "Epoch 21/400\n",
      "42542/42542 [==============================] - 7s 164us/sample - loss: 1.0977 - accuracy: 0.4983\n",
      "Epoch 22/400\n",
      "42542/42542 [==============================] - 7s 163us/sample - loss: 1.0975 - accuracy: 0.4978\n",
      "Epoch 23/400\n",
      "42542/42542 [==============================] - 7s 163us/sample - loss: 1.0947 - accuracy: 0.4996\n",
      "Epoch 24/400\n",
      "42542/42542 [==============================] - 8s 180us/sample - loss: 1.0945 - accuracy: 0.4983\n",
      "Epoch 25/400\n",
      "42542/42542 [==============================] - 7s 163us/sample - loss: 1.0932 - accuracy: 0.4982\n",
      "Epoch 26/400\n",
      "42542/42542 [==============================] - 7s 164us/sample - loss: 1.0925 - accuracy: 0.5025\n",
      "Epoch 27/400\n",
      "42542/42542 [==============================] - 7s 165us/sample - loss: 1.0922 - accuracy: 0.5023\n",
      "Epoch 28/400\n",
      "42542/42542 [==============================] - 7s 165us/sample - loss: 1.0899 - accuracy: 0.5018\n",
      "Epoch 29/400\n",
      "42542/42542 [==============================] - 8s 183us/sample - loss: 1.0884 - accuracy: 0.5049\n",
      "Epoch 30/400\n",
      "42542/42542 [==============================] - 7s 172us/sample - loss: 1.0873 - accuracy: 0.5028\n",
      "Epoch 31/400\n",
      "42542/42542 [==============================] - 7s 171us/sample - loss: 1.0857 - accuracy: 0.5058\n",
      "Epoch 32/400\n",
      "42542/42542 [==============================] - 9s 203us/sample - loss: 1.0835 - accuracy: 0.5046\n",
      "Epoch 33/400\n",
      "42542/42542 [==============================] - 8s 194us/sample - loss: 1.0829 - accuracy: 0.5029\n",
      "Epoch 34/400\n",
      "42542/42542 [==============================] - 7s 170us/sample - loss: 1.0827 - accuracy: 0.5051\n",
      "Epoch 35/400\n",
      "42542/42542 [==============================] - 8s 198us/sample - loss: 1.0832 - accuracy: 0.5043\n",
      "Epoch 36/400\n",
      "42542/42542 [==============================] - 7s 172us/sample - loss: 1.0816 - accuracy: 0.5059\n",
      "Epoch 37/400\n",
      "42542/42542 [==============================] - 7s 174us/sample - loss: 1.0795 - accuracy: 0.5070\n",
      "Epoch 38/400\n",
      "42542/42542 [==============================] - 7s 172us/sample - loss: 1.0806 - accuracy: 0.5058\n",
      "Epoch 39/400\n",
      "42542/42542 [==============================] - 7s 172us/sample - loss: 1.0777 - accuracy: 0.5081\n",
      "Epoch 40/400\n",
      "42542/42542 [==============================] - 9s 203us/sample - loss: 1.0785 - accuracy: 0.5075\n",
      "Epoch 41/400\n",
      "42542/42542 [==============================] - 7s 171us/sample - loss: 1.0757 - accuracy: 0.5087\n",
      "Epoch 42/400\n",
      "42542/42542 [==============================] - 7s 175us/sample - loss: 1.0778 - accuracy: 0.5098\n",
      "Epoch 43/400\n",
      "42542/42542 [==============================] - 7s 173us/sample - loss: 1.0747 - accuracy: 0.5098\n",
      "Epoch 44/400\n",
      "42542/42542 [==============================] - 7s 176us/sample - loss: 1.0740 - accuracy: 0.5106\n",
      "Epoch 45/400\n",
      "42542/42542 [==============================] - 10s 243us/sample - loss: 1.0738 - accuracy: 0.5099\n",
      "Epoch 46/400\n",
      "42542/42542 [==============================] - 10s 244us/sample - loss: 1.0729 - accuracy: 0.5108\n",
      "Epoch 47/400\n",
      "42542/42542 [==============================] - 9s 214us/sample - loss: 1.0715 - accuracy: 0.5106\n",
      "Epoch 48/400\n",
      "42542/42542 [==============================] - 7s 175us/sample - loss: 1.0724 - accuracy: 0.5118\n",
      "Epoch 49/400\n",
      "42542/42542 [==============================] - 8s 184us/sample - loss: 1.0713 - accuracy: 0.5096\n",
      "Epoch 50/400\n",
      "42542/42542 [==============================] - 10s 228us/sample - loss: 1.0701 - accuracy: 0.5119\n",
      "Epoch 51/400\n",
      "42542/42542 [==============================] - 10s 227us/sample - loss: 1.0682 - accuracy: 0.5114\n",
      "Epoch 52/400\n",
      "42542/42542 [==============================] - 8s 181us/sample - loss: 1.0699 - accuracy: 0.5124\n",
      "Epoch 53/400\n",
      "42542/42542 [==============================] - 8s 178us/sample - loss: 1.0670 - accuracy: 0.5138\n",
      "Epoch 54/400\n",
      "42542/42542 [==============================] - 8s 190us/sample - loss: 1.0676 - accuracy: 0.5123\n",
      "Epoch 55/400\n",
      "42542/42542 [==============================] - 8s 177us/sample - loss: 1.0650 - accuracy: 0.5144\n",
      "Epoch 56/400\n",
      "42542/42542 [==============================] - 8s 180us/sample - loss: 1.0662 - accuracy: 0.5129\n",
      "Epoch 57/400\n",
      "42542/42542 [==============================] - 8s 185us/sample - loss: 1.0656 - accuracy: 0.5133\n",
      "Epoch 58/400\n",
      "42542/42542 [==============================] - 8s 182us/sample - loss: 1.0650 - accuracy: 0.5119\n",
      "Epoch 59/400\n",
      "42542/42542 [==============================] - 8s 186us/sample - loss: 1.0642 - accuracy: 0.5140\n",
      "Epoch 60/400\n",
      "42542/42542 [==============================] - 8s 190us/sample - loss: 1.0642 - accuracy: 0.5119\n",
      "Epoch 61/400\n",
      "42542/42542 [==============================] - 7s 175us/sample - loss: 1.0621 - accuracy: 0.5138\n",
      "Epoch 62/400\n",
      "42542/42542 [==============================] - 7s 176us/sample - loss: 1.0614 - accuracy: 0.5133\n",
      "Epoch 63/400\n",
      "42542/42542 [==============================] - 7s 175us/sample - loss: 1.0599 - accuracy: 0.5135\n",
      "Epoch 64/400\n",
      "42542/42542 [==============================] - 8s 178us/sample - loss: 1.0610 - accuracy: 0.5138\n",
      "Epoch 65/400\n",
      "42542/42542 [==============================] - 8s 180us/sample - loss: 1.0595 - accuracy: 0.5149\n",
      "Epoch 66/400\n",
      "42542/42542 [==============================] - 8s 185us/sample - loss: 1.0578 - accuracy: 0.5163\n",
      "Epoch 67/400\n",
      "42542/42542 [==============================] - 8s 191us/sample - loss: 1.0571 - accuracy: 0.5152\n",
      "Epoch 68/400\n",
      "42542/42542 [==============================] - 8s 183us/sample - loss: 1.0583 - accuracy: 0.5154\n",
      "Epoch 69/400\n",
      "42542/42542 [==============================] - 8s 192us/sample - loss: 1.0585 - accuracy: 0.5146\n",
      "Epoch 70/400\n",
      "42542/42542 [==============================] - 8s 194us/sample - loss: 1.0565 - accuracy: 0.5152\n",
      "Epoch 71/400\n",
      "42542/42542 [==============================] - 8s 183us/sample - loss: 1.0591 - accuracy: 0.5158\n",
      "Epoch 72/400\n",
      "42542/42542 [==============================] - 9s 215us/sample - loss: 1.0554 - accuracy: 0.5177\n",
      "Epoch 73/400\n",
      "42542/42542 [==============================] - 9s 200us/sample - loss: 1.0560 - accuracy: 0.5175\n",
      "Epoch 74/400\n",
      "42542/42542 [==============================] - 9s 204us/sample - loss: 1.0571 - accuracy: 0.5167\n",
      "Epoch 75/400\n",
      "42542/42542 [==============================] - 8s 200us/sample - loss: 1.0532 - accuracy: 0.5169\n",
      "Epoch 76/400\n",
      "42542/42542 [==============================] - 15s 342us/sample - loss: 1.0551 - accuracy: 0.5174\n",
      "Epoch 77/400\n",
      "42542/42542 [==============================] - 21s 498us/sample - loss: 1.0547 - accuracy: 0.5186\n",
      "Epoch 78/400\n",
      "42542/42542 [==============================] - 15s 348us/sample - loss: 1.0540 - accuracy: 0.5178\n",
      "Epoch 79/400\n",
      "42542/42542 [==============================] - 12s 282us/sample - loss: 1.0531 - accuracy: 0.5158\n",
      "Epoch 80/400\n",
      "42542/42542 [==============================] - 14s 331us/sample - loss: 1.0542 - accuracy: 0.5173\n",
      "Epoch 81/400\n",
      "42542/42542 [==============================] - 12s 289us/sample - loss: 1.0514 - accuracy: 0.5197\n",
      "Epoch 82/400\n",
      "42542/42542 [==============================] - 21s 495us/sample - loss: 1.0531 - accuracy: 0.5195\n",
      "Epoch 83/400\n",
      "42542/42542 [==============================] - 13s 310us/sample - loss: 1.0522 - accuracy: 0.5176\n",
      "Epoch 84/400\n",
      "42542/42542 [==============================] - 12s 271us/sample - loss: 1.0510 - accuracy: 0.5189\n",
      "Epoch 85/400\n",
      "42542/42542 [==============================] - 11s 258us/sample - loss: 1.0497 - accuracy: 0.5181\n",
      "Epoch 86/400\n",
      "42542/42542 [==============================] - 11s 257us/sample - loss: 1.0512 - accuracy: 0.5173\n",
      "Epoch 87/400\n",
      "42542/42542 [==============================] - 11s 260us/sample - loss: 1.0500 - accuracy: 0.5156\n",
      "Epoch 88/400\n",
      "42542/42542 [==============================] - 11s 266us/sample - loss: 1.0479 - accuracy: 0.5198\n",
      "Epoch 89/400\n",
      "42542/42542 [==============================] - 10s 233us/sample - loss: 1.0505 - accuracy: 0.5224\n",
      "Epoch 90/400\n",
      "42542/42542 [==============================] - 9s 219us/sample - loss: 1.0489 - accuracy: 0.5201\n",
      "Epoch 91/400\n",
      "42542/42542 [==============================] - 9s 207us/sample - loss: 1.0484 - accuracy: 0.5208\n",
      "Epoch 92/400\n",
      "42542/42542 [==============================] - 9s 212us/sample - loss: 1.0486 - accuracy: 0.5184\n",
      "Epoch 93/400\n",
      "42542/42542 [==============================] - 9s 210us/sample - loss: 1.0464 - accuracy: 0.5206\n",
      "Epoch 94/400\n",
      "42542/42542 [==============================] - 9s 213us/sample - loss: 1.0495 - accuracy: 0.5194\n",
      "Epoch 95/400\n",
      "42542/42542 [==============================] - 9s 211us/sample - loss: 1.0455 - accuracy: 0.5224\n",
      "Epoch 96/400\n",
      "42542/42542 [==============================] - 10s 240us/sample - loss: 1.0463 - accuracy: 0.5180\n",
      "Epoch 97/400\n",
      "42542/42542 [==============================] - 10s 240us/sample - loss: 1.0477 - accuracy: 0.5221\n",
      "Epoch 98/400\n",
      "42542/42542 [==============================] - 9s 216us/sample - loss: 1.0453 - accuracy: 0.5197\n",
      "Epoch 99/400\n",
      "42542/42542 [==============================] - 9s 212us/sample - loss: 1.0440 - accuracy: 0.5205\n",
      "Epoch 100/400\n",
      "42542/42542 [==============================] - 9s 217us/sample - loss: 1.0458 - accuracy: 0.5200\n",
      "Epoch 101/400\n",
      "42542/42542 [==============================] - 9s 215us/sample - loss: 1.0443 - accuracy: 0.5195\n",
      "Epoch 102/400\n",
      "42542/42542 [==============================] - 9s 213us/sample - loss: 1.0433 - accuracy: 0.5221\n",
      "Epoch 103/400\n",
      "42542/42542 [==============================] - 10s 230us/sample - loss: 1.0460 - accuracy: 0.5199\n",
      "Epoch 104/400\n",
      "42542/42542 [==============================] - 9s 222us/sample - loss: 1.0427 - accuracy: 0.5202\n",
      "Epoch 105/400\n",
      "42542/42542 [==============================] - 9s 211us/sample - loss: 1.0445 - accuracy: 0.5182\n",
      "Epoch 106/400\n",
      "42542/42542 [==============================] - 9s 209us/sample - loss: 1.0434 - accuracy: 0.5218\n",
      "Epoch 107/400\n",
      "42542/42542 [==============================] - 9s 215us/sample - loss: 1.0441 - accuracy: 0.5191\n",
      "Epoch 108/400\n",
      "42542/42542 [==============================] - 9s 208us/sample - loss: 1.0425 - accuracy: 0.5227\n",
      "Epoch 109/400\n",
      "42542/42542 [==============================] - 9s 208us/sample - loss: 1.0415 - accuracy: 0.5207\n",
      "Epoch 110/400\n",
      "42542/42542 [==============================] - 9s 210us/sample - loss: 1.0404 - accuracy: 0.5214\n",
      "Epoch 111/400\n",
      "42542/42542 [==============================] - 9s 213us/sample - loss: 1.0430 - accuracy: 0.5218\n",
      "Epoch 112/400\n",
      "42542/42542 [==============================] - 9s 210us/sample - loss: 1.0429 - accuracy: 0.5231\n",
      "Epoch 113/400\n",
      "42542/42542 [==============================] - 9s 212us/sample - loss: 1.0413 - accuracy: 0.5263\n",
      "Epoch 114/400\n",
      "42542/42542 [==============================] - 9s 213us/sample - loss: 1.0403 - accuracy: 0.5212\n",
      "Epoch 115/400\n",
      "42542/42542 [==============================] - 10s 226us/sample - loss: 1.0397 - accuracy: 0.5243\n",
      "Epoch 116/400\n",
      "42542/42542 [==============================] - 12s 287us/sample - loss: 1.0409 - accuracy: 0.5226\n",
      "Epoch 117/400\n",
      "42542/42542 [==============================] - 10s 240us/sample - loss: 1.0406 - accuracy: 0.5246\n",
      "Epoch 118/400\n",
      "42542/42542 [==============================] - 9s 220us/sample - loss: 1.0383 - accuracy: 0.5236\n",
      "Epoch 119/400\n",
      "42542/42542 [==============================] - 9s 217us/sample - loss: 1.0401 - accuracy: 0.5243\n",
      "Epoch 120/400\n",
      "42542/42542 [==============================] - 9s 215us/sample - loss: 1.0396 - accuracy: 0.5235\n",
      "Epoch 121/400\n",
      "42542/42542 [==============================] - 9s 217us/sample - loss: 1.0380 - accuracy: 0.5245\n",
      "Epoch 122/400\n",
      "42542/42542 [==============================] - 9s 223us/sample - loss: 1.0375 - accuracy: 0.5233\n",
      "Epoch 123/400\n",
      "42542/42542 [==============================] - 8s 192us/sample - loss: 1.0377 - accuracy: 0.5217\n",
      "Epoch 124/400\n",
      "42542/42542 [==============================] - 8s 193us/sample - loss: 1.0358 - accuracy: 0.5249\n",
      "Epoch 125/400\n",
      "42542/42542 [==============================] - 8s 191us/sample - loss: 1.0374 - accuracy: 0.5254\n",
      "Epoch 126/400\n",
      "42542/42542 [==============================] - 8s 194us/sample - loss: 1.0367 - accuracy: 0.5251\n",
      "Epoch 127/400\n",
      "42542/42542 [==============================] - 8s 194us/sample - loss: 1.0363 - accuracy: 0.5245\n",
      "Epoch 128/400\n",
      "42542/42542 [==============================] - 8s 189us/sample - loss: 1.0361 - accuracy: 0.5265\n",
      "Epoch 129/400\n",
      "42542/42542 [==============================] - 9s 202us/sample - loss: 1.0355 - accuracy: 0.5247\n",
      "Epoch 130/400\n",
      "42542/42542 [==============================] - 8s 192us/sample - loss: 1.0374 - accuracy: 0.5232\n",
      "Epoch 131/400\n",
      "42542/42542 [==============================] - 9s 216us/sample - loss: 1.0367 - accuracy: 0.5245\n",
      "Epoch 132/400\n",
      "42542/42542 [==============================] - 8s 193us/sample - loss: 1.0343 - accuracy: 0.5297\n",
      "Epoch 133/400\n",
      "42542/42542 [==============================] - 8s 193us/sample - loss: 1.0346 - accuracy: 0.5262\n",
      "Epoch 134/400\n",
      "42542/42542 [==============================] - 8s 197us/sample - loss: 1.0362 - accuracy: 0.5247\n",
      "Epoch 135/400\n",
      "42542/42542 [==============================] - 8s 191us/sample - loss: 1.0326 - accuracy: 0.5264\n",
      "Epoch 136/400\n",
      "42542/42542 [==============================] - 9s 212us/sample - loss: 1.0343 - accuracy: 0.5260\n",
      "Epoch 137/400\n",
      "42542/42542 [==============================] - 8s 195us/sample - loss: 1.0339 - accuracy: 0.5239\n",
      "Epoch 138/400\n",
      "42542/42542 [==============================] - 8s 193us/sample - loss: 1.0323 - accuracy: 0.5255\n",
      "Epoch 139/400\n",
      "42542/42542 [==============================] - 8s 191us/sample - loss: 1.0314 - accuracy: 0.5272\n",
      "Epoch 140/400\n",
      "42542/42542 [==============================] - 8s 196us/sample - loss: 1.0332 - accuracy: 0.5257\n",
      "Epoch 141/400\n",
      "42542/42542 [==============================] - 9s 214us/sample - loss: 1.0324 - accuracy: 0.5265\n",
      "Epoch 142/400\n",
      "42542/42542 [==============================] - 8s 194us/sample - loss: 1.0347 - accuracy: 0.5255\n",
      "Epoch 143/400\n",
      "42542/42542 [==============================] - 9s 209us/sample - loss: 1.0327 - accuracy: 0.5274\n",
      "Epoch 144/400\n",
      "42542/42542 [==============================] - 8s 194us/sample - loss: 1.0336 - accuracy: 0.5267\n",
      "Epoch 145/400\n",
      "42542/42542 [==============================] - 8s 196us/sample - loss: 1.0351 - accuracy: 0.5273\n",
      "Epoch 146/400\n",
      "42542/42542 [==============================] - 8s 194us/sample - loss: 1.0313 - accuracy: 0.5279\n",
      "Epoch 147/400\n",
      "42542/42542 [==============================] - 8s 191us/sample - loss: 1.0330 - accuracy: 0.5273\n",
      "Epoch 148/400\n",
      "42542/42542 [==============================] - 9s 205us/sample - loss: 1.0324 - accuracy: 0.5292\n",
      "Epoch 149/400\n",
      "42542/42542 [==============================] - 8s 192us/sample - loss: 1.0305 - accuracy: 0.5270\n",
      "Epoch 150/400\n",
      "42542/42542 [==============================] - 8s 192us/sample - loss: 1.0338 - accuracy: 0.5269\n",
      "Epoch 151/400\n",
      "42542/42542 [==============================] - 8s 193us/sample - loss: 1.0322 - accuracy: 0.5278\n",
      "Epoch 152/400\n",
      "42542/42542 [==============================] - 9s 217us/sample - loss: 1.0306 - accuracy: 0.5291\n",
      "Epoch 153/400\n",
      "42542/42542 [==============================] - 11s 264us/sample - loss: 1.0285 - accuracy: 0.5303\n",
      "Epoch 154/400\n",
      "42542/42542 [==============================] - 9s 209us/sample - loss: 1.0320 - accuracy: 0.5254\n",
      "Epoch 155/400\n",
      "42542/42542 [==============================] - 8s 195us/sample - loss: 1.0299 - accuracy: 0.5298\n",
      "Epoch 156/400\n",
      "42542/42542 [==============================] - 8s 192us/sample - loss: 1.0311 - accuracy: 0.5294\n",
      "Epoch 157/400\n",
      "42542/42542 [==============================] - 9s 211us/sample - loss: 1.0290 - accuracy: 0.5291\n",
      "Epoch 158/400\n",
      "42542/42542 [==============================] - 8s 195us/sample - loss: 1.0278 - accuracy: 0.5298\n",
      "Epoch 159/400\n",
      "42542/42542 [==============================] - 9s 223us/sample - loss: 1.0276 - accuracy: 0.5311\n",
      "Epoch 160/400\n",
      "42542/42542 [==============================] - 8s 196us/sample - loss: 1.0293 - accuracy: 0.5273\n",
      "Epoch 161/400\n",
      "42542/42542 [==============================] - 9s 206us/sample - loss: 1.0283 - accuracy: 0.5287\n",
      "Epoch 162/400\n",
      "42542/42542 [==============================] - 9s 218us/sample - loss: 1.0292 - accuracy: 0.5298\n",
      "Epoch 163/400\n",
      "42542/42542 [==============================] - 9s 206us/sample - loss: 1.0294 - accuracy: 0.5281\n",
      "Epoch 164/400\n",
      "42542/42542 [==============================] - 10s 244us/sample - loss: 1.0303 - accuracy: 0.5271\n",
      "Epoch 165/400\n",
      "42542/42542 [==============================] - 9s 202us/sample - loss: 1.0263 - accuracy: 0.5277\n",
      "Epoch 166/400\n",
      "42542/42542 [==============================] - 8s 179us/sample - loss: 1.0282 - accuracy: 0.5272\n",
      "Epoch 167/400\n",
      "42542/42542 [==============================] - 8s 194us/sample - loss: 1.0275 - accuracy: 0.5283\n",
      "Epoch 168/400\n",
      "42542/42542 [==============================] - 9s 215us/sample - loss: 1.0275 - accuracy: 0.5262\n",
      "Epoch 169/400\n",
      "42542/42542 [==============================] - 9s 215us/sample - loss: 1.0299 - accuracy: 0.5273\n",
      "Epoch 170/400\n",
      "42542/42542 [==============================] - 9s 218us/sample - loss: 1.0260 - accuracy: 0.5279\n",
      "Epoch 171/400\n",
      "42542/42542 [==============================] - 8s 190us/sample - loss: 1.0270 - accuracy: 0.5297\n",
      "Epoch 172/400\n",
      "42542/42542 [==============================] - 8s 177us/sample - loss: 1.0252 - accuracy: 0.5296\n",
      "Epoch 173/400\n",
      "42542/42542 [==============================] - 9s 203us/sample - loss: 1.0272 - accuracy: 0.5268\n",
      "Epoch 174/400\n",
      "42542/42542 [==============================] - 8s 183us/sample - loss: 1.0271 - accuracy: 0.5318\n",
      "Epoch 175/400\n",
      "42542/42542 [==============================] - 8s 187us/sample - loss: 1.0279 - accuracy: 0.5277\n",
      "Epoch 176/400\n",
      "42542/42542 [==============================] - 8s 200us/sample - loss: 1.0260 - accuracy: 0.5296\n",
      "Epoch 177/400\n",
      "42542/42542 [==============================] - 8s 183us/sample - loss: 1.0272 - accuracy: 0.5270\n",
      "Epoch 178/400\n",
      "42542/42542 [==============================] - 8s 185us/sample - loss: 1.0282 - accuracy: 0.5267\n",
      "Epoch 179/400\n",
      "42542/42542 [==============================] - 8s 194us/sample - loss: 1.0257 - accuracy: 0.5316\n",
      "Epoch 180/400\n",
      "42542/42542 [==============================] - 8s 192us/sample - loss: 1.0243 - accuracy: 0.5303\n",
      "Epoch 181/400\n",
      "42542/42542 [==============================] - 7s 174us/sample - loss: 1.0250 - accuracy: 0.5296\n",
      "Epoch 182/400\n",
      "42542/42542 [==============================] - 7s 168us/sample - loss: 1.0261 - accuracy: 0.5307\n",
      "Epoch 183/400\n",
      "42542/42542 [==============================] - 7s 167us/sample - loss: 1.0277 - accuracy: 0.5291\n",
      "Epoch 184/400\n",
      "42542/42542 [==============================] - 7s 168us/sample - loss: 1.0249 - accuracy: 0.5307\n",
      "Epoch 185/400\n",
      "42542/42542 [==============================] - 8s 187us/sample - loss: 1.0266 - accuracy: 0.5280\n",
      "Epoch 186/400\n",
      "17184/42542 [===========>..................] - ETA: 5s - loss: 1.0261 - accuracy: 0.5239"
     ]
    }
   ],
   "source": [
    "classifier_ffn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_std.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(y_ctrain.shape[1], activation=tf.nn.softmax)\n",
    "])\n",
    "classifier_ffn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier_ffn.fit(X_train_std, df_train['target_cat'].to_numpy(), epochs=400)\n",
    "\n",
    "y_pred_ffn = classifier_ffn.predict(X_test_std)\n",
    "y_pred_ffn = pd.get_dummies(y_pred_ffn.argmax(axis=1))\n",
    "print(y_pred_ffn.sum(axis=0))\n",
    "score_classifier(y_ctest, y_pred_ffn.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "The other possible option is regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-stocks",
   "language": "python",
   "name": "ds-stocks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
